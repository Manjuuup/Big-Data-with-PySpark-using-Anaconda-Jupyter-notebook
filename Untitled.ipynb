{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a5ea3d-7544-4eb7-bdf7-b8fb16c02ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946d2000-2d49-42aa-a2f5-72a6a0f4a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/02 15:00:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/02 15:00:31 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(\"data/*\")\n",
    "csv_df.write.mode(\"overwrite\").parquet(\"friends1.parquet\")\n",
    "parquet_df = spark.read.parquet(\"friends1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b346555e-5e8f-49f8-8a14-a0434ae4a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = csv_df.withColumn('MAX',col('MAX').cast('double'))\n",
    "csv_df.createOrReplaceTempView(\"temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b187ea-9b78-4096-bcdf-a58a453c2a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+----+\n",
      "|    station|                name|year(date)| MAX|\n",
      "+-----------+--------------------+----------+----+\n",
      "|99407099999|DESTRUCTION IS. W...|      2010|74.8|\n",
      "|01046099999|       SORKJOSEN, NO|      2011|87.8|\n",
      "|01023099999|       BARDUFOSS, NO|      2012|72.0|\n",
      "|01001499999|      SORSTOKKEN, NO|      2013|80.6|\n",
      "|01023099999|       BARDUFOSS, NO|      2014|89.6|\n",
      "|01025099999|          TROMSO, NO|      2015|71.6|\n",
      "|01023199999|         DRAUGEN, NO|      2016|77.0|\n",
      "|01023099999|       BARDUFOSS, NO|      2017|78.6|\n",
      "|01025099999|          TROMSO, NO|      2018|84.2|\n",
      "|01023099999|       BARDUFOSS, NO|      2019|78.8|\n",
      "|01023099999|       BARDUFOSS, NO|      2019|78.8|\n",
      "|01023099999|       BARDUFOSS, NO|      2020|79.9|\n",
      "|01065099999|        KARASJOK, NO|      2021|88.3|\n",
      "|02095099999|          PAJALA, SW|      2022|85.5|\n",
      "+-----------+--------------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "querya=spark.sql(\"select station, name, year(date) ,MAX from temp_view where (year(date),MAX) in (select year(date),max(MAX) from temp_view where max!=9999.9 group by year(date)) order by year(date)\")\n",
    "querya.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7c1f2-f605-4fad-b2a4-9727d4b48a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14a1236b-7ab1-422b-b650-9f16d7d67e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+-----+------+\n",
      "|    station|         name|year|month|   min|\n",
      "+-----------+-------------+----+-----+------+\n",
      "|01023099999|BARDUFOSS, NO|2017|    1| -28.3|\n",
      "+-----------+-------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryb=spark.sql(\"select station, name, year(date) as year,month(date) as month, min as min from temp_view where min = (select min(int(min)) from temp_view where month(date)=1) limit 1\")\n",
    "queryb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0a0fd2-6fcf-4675-8726-e34f3baaf5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+\n",
      "|    station|      name|      date|max_prcp|\n",
      "+-----------+----------+----------+--------+\n",
      "|01025099999|TROMSO, NO|2015-11-02|    2.11|\n",
      "+-----------+----------+----------+--------+\n",
      "\n",
      "+-----------+------------+----------+--------+\n",
      "|    station|        name|      date|min_prcp|\n",
      "+-----------+------------+----------+--------+\n",
      "|01008099999|LONGYEAR, SV|2015-01-18|    0.01|\n",
      "+-----------+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryca=spark.sql(\"select station,name,date,prcp as max_prcp from temp_view where year(date)=2015 and prcp=(select max(prcp) from temp_view where year(date)=2015  and prcp!=99.99) limit 1\")\n",
    "queryca.show()\n",
    "querycb=spark.sql(\"select station,name,date,prcp as min_prcp from temp_view where year(date)=2015 and prcp=(select min(prcp) from temp_view where year(date)=2015  and prcp!=0.00) limit 1\")\n",
    "querycb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442b811e-cec1-448c-8491-d91e9e10bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|percentage_missing_gust|\n",
      "+-----------------------+\n",
      "|      82.87671232876713|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryd=spark.sql(\"Select (Select count(*) from temp_view where year(date)=2019 and gust=999.9)*100/(Select count(*) from temp_view where year(date)=2019 ) as percentage_missing_gust from temp_view limit 1\")\n",
    "queryd.show(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754ed01-8766-42d1-b6da-8ab4929de578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3aa7f3-a71f-44cf-99b7-802ad01e76ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8427d16-5385-4df8-ae2b-c304f4cbaa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b90b869-2fad-4950-8a49-7e3c54704574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------+------------------+---------+\n",
      "|month|         mean_temp|median_temp|        stdev_temp|mode_temp|\n",
      "+-----+------------------+-----------+------------------+---------+\n",
      "|    1|15.896774193548387|       14.9|12.805172721989297|      5.7|\n",
      "|    2| 13.35862068965517|       15.3| 13.09180853418292|     15.5|\n",
      "|    3|14.653225806451612|       18.6|15.784789500893567|     35.4|\n",
      "|    4|23.329999999999995|       26.0| 13.02209725617009|     34.1|\n",
      "|    5| 36.21935483870967|       36.0| 8.077246704851957|     37.0|\n",
      "|    6| 47.42999999999999|       46.0| 8.877190347997287|     58.6|\n",
      "|    7| 52.88709677419356|       51.4|  6.66378723291517|     49.3|\n",
      "|    8|49.287096774193564|       48.7| 6.548594740281946|     44.7|\n",
      "|    9| 41.84499999999999|       42.5| 5.887660897797832|     46.5|\n",
      "|   10|31.529032258064507|       30.7| 9.609052888228815|     32.2|\n",
      "|   11|29.246666666666666|       29.8| 8.143448373534971|     36.3|\n",
      "|   12| 19.95483870967743|       20.2| 8.854464048157649|     25.3|\n",
      "+-----+------------------+-----------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a= spark.sql(\"with temp_stats as ( select month(date) as month, avg(temp) as mean_temp, percentile_approx(temp, 0.5) as median_temp, stddev(temp) as stdev_temp from temp_view where year(date) = 2020 group by month ), temp_mode as ( with temp_count as ( select temp, month(date) as month, count(temp) over (partition by temp, month(date)) as cnt from temp_view where year(date) = 2020 ), max_count as ( select distinct month, max(cnt) over(partition by MONTH) as mx from temp_count ) select distinct max_count.month, max(temp) over (partition by max_count.month) as mode_temp from temp_count, max_count where temp_count.MONTH=max_count.MONTH and temp_count.cnt=max_count.mx order by max_count.MONTH ) select ts.month, ts.mean_temp, ts.median_temp, ts.stdev_temp, tm.mode_temp from temp_stats ts join temp_mode tm on ts.month = tm.month order by ts.month\")\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd5942-e181-4d37-a6f5-979956c09c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
